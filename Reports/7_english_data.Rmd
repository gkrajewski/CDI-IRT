---
title: "Models for English comprehension"
author: "Piotr Kr√≥l"
date: "19 07 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
```

In this report I try to replicate Kachergis et al. model for English comprehension by obtaining data from Wordbank.

```{r libraries, warning=FALSE, message=FALSE}
library(wordbankr)
library(mirt)
```
```{r include=F}
load("Data/7_english_data.RData")
```

Obtain English WG comprehension data from Wordbank:

```{r}
english_wg <- get_item_data("English (American)", "WG")
words_items <- english_wg[english_wg$type == "word", "item_id"]
data <- get_instrument_data(
  language = "English (American)",
  form = "WG",
  items = words_items$item_id
)
data <- as.data.frame(data)
data <- na.omit(data)
```

Prepare data for mirt (1 = understands, 0 = not)

```{r}
data[data$value == "produces" | data$value == "understands", "value"] <- "1"
data[data$value == "", "value"] <- "0"
```

Prepare responses matrix for mirt:

```{r}
responses <- reshape(data, 
                   v.names = "value",
                   timevar = "num_item_id",
                   idvar = c("data_id"),
                   direction = "wide"
                   )
```

```{r}
responses_m <- as.matrix(responses[,2:ncol(responses)])
colnames(responses_m) <- paste0("item", c(1:396))
responses_m <- apply(responses_m, 2, as.numeric) #Convert strings "1" and "0" to numbers 1 and 0
```

Create model with default settings:

```{r eval=F}
mirt_model1 <- mirt(responses_m, 1)
```
```{r}
mirt_model1
```
Compare to Kachergis et al. model:

```{r}
mod_2pl
```
Check once more with their settings (add this itemtype = "2PL"): 

```{r eval=F}
mirt_model2 <- mirt(responses_m, model = 1, itemtype = "2PL")
```
```{r}
mirt_model2
```
No change. It is the same as mirt_model1. Let's compare parameters. My parameters:

```{r}
my_params <- as.data.frame(coef(mirt_model1, simplify = TRUE)$items)[1:2]
head(my_params)
```

Their parameters:

```{r}
head(coefs_2pl)
```

Check ceiling and floor subjects:

```{r}
length(which(rowSums(responses_m) == 0))
```
8 floor subjects

```{r}
length(which(rowSums(responses_m) == ncol(responses_m)))
```
5 ceiling subjects

Let's remove them and create model once more:

```{r}
subjects_to_rm <- c(which(rowSums(responses_m) == 0), which(rowSums(responses_m) == ncol(responses_m)))
responses_m_2 <- responses_m[-subjects_to_rm,]
```

```{r eval=F}
mirt_model3 <- mirt(responses_m_2, model = 1, itemtype = "2PL")
```
```{r}
mirt_model3
```
Still I can't obtain as low log-likelihood as theirs. Also their model has 2x iterations as mine.

# Spanish comprehension

Obtain Spanish comprehension data from Wordbank:

```{r}
spanish_wg <- get_item_data("Spanish (Mexican)", "WG")
Encoding(spanish_wg$definition) <- "UTF-8"
words_items <- spanish_wg[spanish_wg$type == "word", "item_id"]
data <- get_instrument_data(
  language = "Spanish (Mexican)",
  form = "WG",
  items = words_items$item_id
)

#Prepare data for mirt (1 = understands, 0 = not)
data <- as.data.frame(data)
data <- na.omit(data)
data[data$value == "produces" | data$value == "understands", "value"] <- "1"
data[data$value == "", "value"] <- "0"

#Prepare responses matrix for mirt:
responses <- reshape(data, 
                   v.names = "value",
                   timevar = "num_item_id",
                   idvar = c("data_id"),
                   direction = "wide"
                   )
responses_m <- as.matrix(responses[,2:ncol(responses)])
colnames(responses_m) <- paste0("item", c(1:nrow(words_items)))
responses_m <- apply(responses_m, 2, as.numeric) #Convert strings "1" and "0" to numbers 1 and 0
```

```{r include=F}
mod_spanish <- mirt(responses_m, 1)
```

```{r}
mod_spanish
```
```{r}
load("Data/sp_wg_mod_2pl_nobad.Rds")
```

```{r}
mod_2pl
```




